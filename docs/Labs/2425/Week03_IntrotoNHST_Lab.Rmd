---
title: "Week 3: Intro to NHST"
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
    css: https://mtruelovehill.github.io/PRM/Labs/css/style.css
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)

pacman::p_load('tidyverse', 'rstatix')
# library(tidyverse)
# library(kableExtra)
# library(knitr)
# library(shiny)
# library(shinyjs)
# library(bslib)
# library(pwr)

baseColor <- '#4CA384'
accent1 <- '#9AD079'
accent2 <- '#C4C6C7'
accent3 <- '#19424C'

dat <- read.csv('https://mtruelovehill.github.io/PRM/Labs/Week1LabData.csv')
studentDat <- dat[dat$Role=='Student',]
studentDat$Cperc <- (studentDat$C/50)*100
Cdens <- data.frame(cPerc=density(studentDat$Cperc, from = 0, to = 100)$x, dens=density(studentDat$Cperc, from = 0, to = 100)$y)


```

```{r, context = 'server'}
output$aucPlot <- renderPlot({
  loLim <- ifelse(input$auc[1]==0, .00001, input$auc[1])
  hiLim <- ifelse(input$auc[2]==1, .999999, input$auc[2])
  
  ggplot(data.frame(x=c(-6, 6)), aes(x)) + 
    stat_function(fun=dnorm, geom = "line", linewidth = 1.5) +
    geom_area(stat = "function", fun = dnorm, fill = baseColor, xlim = c(qnorm(loLim), qnorm(hiLim)),
              alpha = .7) + 
    theme(axis.text = element_blank(),
          axis.ticks = element_blank(),
          axis.title = element_blank()) +
    annotate('text', label = paste0((input$auc[2]-input$auc[1])*100, '% of observations'),
             x = -3, y = .3, size = 6)
    })



output$aucScorePlot <- renderPlot({
  loLim <- ifelse(input$score[1]==31, 31, input$score[1])
  hiLim <- ifelse(input$score[2]==84, 84, input$score[2])


  propVal <- round((pnorm(input$score[2], mean = 58, sd = 9.4)-pnorm(input$score[1], mean = 58, sd = 9.4))*100)

  ggplot(data.frame(x=c(31, 84)), aes(x)) +
    stat_function(fun=dnorm, geom = "line", linewidth = 1.5, args = c(mean = 58, sd = 9)) +
    geom_area(stat = "function", fun = dnorm, fill = baseColor, args = c(mean = 58, sd = 9), xlim = c(loLim, hiLim),
              alpha = .7) +
    theme(axis.text = element_blank(),
          axis.ticks = element_blank(),
          axis.title = element_blank()) +
    annotate('text', label = paste0(propVal, '% of observations'), x = 42, y = .03, size = 6)
    })


output$alphaPlot <- renderPlot({
  ggplot(data.frame(x=c(-5, 5)), aes(x)) +
    stat_function(fun=dnorm, geom = "line", linewidth = 1.5, args = c(mean = 0, sd = 1)) + 
    geom_area(stat = "function", fun = dnorm, fill = accent1, args = c(mean = 0, sd = 1),
              xlim = c(qnorm(1-(as.numeric(input$alpha)/2)), 5), alpha = .7) +
    geom_area(stat = "function", fun = dnorm, fill = accent1, args = c(mean = 0, sd = 1),
              xlim = c(-5, qnorm(1-(as.numeric(input$alpha)/2), lower.tail = F)), alpha = .7) +
    scale_x_continuous(breaks = seq(-5, 5, by = .5)) +
    xlab('test statistic') +
    geom_vline(xintercept = qnorm(1-(as.numeric(input$alpha)/2), lower.tail = F), color = baseColor, linewidth = 1.5, linetype='dashed') +
    geom_vline(xintercept = qnorm(1-(as.numeric(input$alpha)/2)), color = baseColor, linewidth = 1.5, linetype='dashed') +
    theme(axis.text.y = element_blank(),
          axis.ticks.y = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_text(size = 14, face = 'bold'),
          axis.title.x = element_text(size = 16, face = 'bold'))
})

output$pPlot <- renderPlot({
  lowProb <- round(pnorm(input$testStatistic, mean = 0, sd = 1), 4)*100
  hiProb <- round(pnorm(input$testStatistic, mean = 0, sd = 1, lower.tail = F),4)*100

  limVal <- ifelse(input$testStatistic<0, input$testStatistic, input$testStatistic*-1)

  pVal <- ifelse(input$testStatistic<0, round(pnorm(input$testStatistic, mean = 0, sd = 1), 3)*2,
                 round(pnorm(input$testStatistic, mean = 0, sd = 1, lower.tail = F), 3)*2)

  ggplot(data.frame(x=c(-5, 5)), aes(x)) +
    geom_area(stat = "function", fun = dnorm, fill = baseColor, args = c(mean = 0, sd = 1),
              xlim = c(-5, limVal), alpha = .7) +
    geom_area(stat = "function", fun = dnorm, fill = baseColor, args = c(mean = 0, sd = 1),
              xlim = c(abs(limVal), 5), alpha = .7) +
    geom_area(stat = "function", fun = dnorm, fill = accent1, args = c(mean = 0, sd = 1),
              xlim = c(limVal, abs(limVal)), alpha = .7) +
    stat_function(fun=dnorm, geom = "line", linewidth = 1.5, args = c(mean = 0, sd = 1)) +
    scale_x_continuous(breaks = seq(-5, 5, by = 1)) +
    scale_y_continuous(limits=c(0, .4)) +
    xlab('test statistic') +
    geom_vline(xintercept = input$testStatistic, color = accent3, linewidth = 1.5, linetype='dashed') +
    theme(axis.text.y = element_blank(),
          axis.ticks.y = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_text(size = 14, face = 'bold'),
          axis.title.x = element_text(size = 16, face = 'bold')) +
    geom_rect(aes(xmin = -3.8, xmax = -2.7, ymin = .03, ymax = .07), fill = baseColor, alpha = .5) +
    geom_rect(aes(xmin = 3.8, xmax = 2.7, ymin = .03, ymax = .07), fill = baseColor, alpha = .5) +
    annotate('text', label = paste0('P', ifelse(pVal==0, ' < .001', paste0(' = ', pVal/2))), x = 3.25, y = .05, size = 6) +
    annotate('text', label = paste0('P', ifelse(pVal==0, ' < .001', paste0(' = ', pVal/2))), x = -3.25, y = .05, size = 6)
})

# output$pPlot <- renderPlot({
#   lowProb <- round(pnorm(input$testStatistic, mean = 0, sd = 1), 4)*100
#   hiProb <- round(pnorm(input$testStatistic, mean = 0, sd = 1, lower.tail = F),4)*100
#   
#   limVal <- ifelse(input$testStatistic<0, input$testStatistic, input$testStatistic*-1)
#   
#   pVal <- ifelse(input$testStatistic<0, round(pnorm(input$testStatistic, mean = 0, sd = 1), 3)*2, 
#                  round(pnorm(input$testStatistic, mean = 0, sd = 1, lower.tail = F), 3)*2)
#   
#   ggplot(data.frame(x=c(-5, 5)), aes(x)) +
#     geom_area(stat = "function", fun = dnorm, fill = baseColor, args = c(mean = 0, sd = 1),
#               xlim = c(-5, limVal), alpha = .7) +
#     geom_area(stat = "function", fun = dnorm, fill = baseColor, args = c(mean = 0, sd = 1),
#               xlim = c(abs(limVal), 5), alpha = .7) +
#     geom_area(stat = "function", fun = dnorm, fill = accent1, args = c(mean = 0, sd = 1),
#               xlim = c(limVal, abs(limVal)), alpha = .7) +
#     stat_function(fun=dnorm, geom = "line", linewidth = 1.5, args = c(mean = 0, sd = 1)) +
#     scale_x_continuous(breaks = seq(-5, 5, by = 1)) +
#     scale_y_continuous(limits=c(0, .4)) +
#     xlab('test statistic') +
#     geom_vline(xintercept = input$testStatistic, color = accent3, linewidth = 1.5, linetype='dashed') +
#     theme(axis.text.y = element_blank(),
#           axis.ticks.y = element_blank(),
#           axis.title.y = element_blank(),
#           axis.text.x = element_text(size = 14, face = 'bold'),
#           axis.title.x = element_text(size = 16, face = 'bold')) +
#     geom_rect(aes(xmin = -3.8, xmax = -2.7, ymin = .03, ymax = .07), fill = baseColor, alpha = .5) +
#     geom_rect(aes(xmin = 1.2, xmax = 2.3, ymin = .28, ymax = .32), fill = accent1, alpha = .5) +
#     geom_rect(aes(xmin = 3.8, xmax = 2.7, ymin = .03, ymax = .07), fill = baseColor, alpha = .5) +
#     annotate('text', label = paste0('p', ifelse(pVal==0, ' < .001', paste0(' = ', pVal/2))), x = 3.25, y = .05, size = 6) +
#     annotate('text', label = paste0('p', ifelse(pVal==0, ' < .001', paste0(' = ', pVal/2))), x = -3.25, y = .05, size = 6) +
#     annotate('text', label = paste0('p', ifelse(pVal==0, ' < .001', paste0(' = ', 1-pVal))), x = 1.75, y = .3, size = 6)
# })



output$testPlot <- renderPlot({
  limVal <- ifelse(input$testStat2<0, input$testStat2, input$testStat2*-1)
  
  pVal <- ifelse(input$testStat2<0, round(pnorm(input$testStat2, mean = 0, sd = 1), 3)*2, 
                 round(pnorm(input$testStat2, mean = 0, sd = 1, lower.tail = F), 3)*2)
  
  ggplot(data.frame(x=c(-5, 5)), aes(x)) +
    geom_area(stat = "function", fun = dnorm, fill = baseColor, args = c(mean = 0, sd = 1),
              xlim = c(-5, limVal), alpha = .7) +
    geom_area(stat = "function", fun = dnorm, fill = baseColor, args = c(mean = 0, sd = 1),
              xlim = c(abs(limVal), 5), alpha = .7) +
    stat_function(fun=dnorm, geom = "line", linewidth = 1.5, args = c(mean = 0, sd = 1)) +
    scale_x_continuous(breaks = seq(-5, 5, by = 1)) +
    xlab('test statistic') +
    geom_vline(xintercept = input$testStat2, color = accent3, linewidth = 1.5, linetype='dashed') +
    theme(axis.text.y = element_blank(),
          axis.ticks.y = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_text(size = 14, face = 'bold'),
          axis.title.x = element_text(size = 16, face = 'bold')) +
    annotate('text', label = paste0('p', ifelse(pVal==0, ' < .001', paste0(' = ', pVal))), x = 3.25, y = .2, size = 6)
})

output$effectSizePlot <- renderPlot({
  
  set.seed(820)
  G1 <- rnorm(as.numeric(input$n), mean = 50, sd = input$sdG1)
  testDat <- data.frame(Group = c(rep('G1', as.numeric(input$n)), rep('G2', as.numeric(input$n))), 
                        x = c(G1, G1+as.numeric(input$meanDiff)))
  
  pVal <- round(t.test(x~Group, data = testDat)$p.value,3)
  effSize <- as.numeric(round(cohens_d(testDat, x~Group)$effsize,2))
  
  ggplot(data.frame(x=c(15, 85)), aes(x)) +
    geom_histogram(data = testDat[testDat$Group=='G1',], aes(x, y = after_stat(density)), binwidth = 2, fill = baseColor, color = accent3, alpha = .7) +
    geom_histogram(data = testDat[testDat$Group=='G2',], aes(x, y = after_stat(density)), binwidth = 2, fill = accent1, color = accent3, alpha = .7) +
    geom_density(data = testDat[testDat$Group=='G1',], aes(x), color = accent3, linewidth = 1) +
    geom_density(data = testDat[testDat$Group=='G2',], aes(x), color = accent3, linewidth = 1) +
    scale_x_continuous(limits = c(15, 85), breaks = seq(15, 85, by = 10)) +
    theme(axis.text.x = element_text(size = 12),
          axis.title.x = element_text(size = 14, face = 'bold'),
          axis.text.y = element_blank(),
          axis.title.y = element_blank(),
          axis.ticks.y = element_blank()) +
    annotate(geom = 'text', label = paste('Effect Size =', abs(effSize)), x = 25, y = .19, size = 6) +
    annotate(geom = 'text', label = paste('p', ifelse(pVal==0, ' < .001', paste0(' = ', pVal))), 
             x = 25, y = .16, size = 6)
  
})


  observeEvent(input$do, {
    CIs <- data.frame(Sample = 1:20, mean = rep(NA,20), lowBound = rep(NA, 20), hiBound = rep(NA, 20))
    ciDat <- read.csv('https://mtruelovehill.github.io/PRM/Data/ciDat.csv')
    
    for(x in 1:nrow(CIs)) {
      sampDat <- sample(ciDat$Cscore, size = 50)
      margin <- 1.96*sd(sampDat)/sqrt(50)
      CIs$mean[x] <- round(mean(sampDat), 2)
      CIs$lowBound[x] <- mean(sampDat)-margin
      CIs$hiBound[x] <- mean(sampDat)+margin
    }
    
    CIs$plotColors <- ifelse(mean(ciDat$Cscore)>=CIs$lowBound&mean(ciDat$Cscore)<=CIs$hiBound, baseColor, '#D83535')
    
    output$ciPlot <- renderPlot({
      ggplot(CIs, aes(mean, y = Sample)) +
        geom_errorbar(data=CIs, aes(xmin = lowBound, xmax = hiBound, y = Sample), width=0.2, linewidth=1, 
                    color=CIs$plotColors) + 
        geom_point(color = CIs$plotColors) +
        labs(x = 'Score', y = 'Sample Number') +
        scale_x_continuous(limits = c(45, 75), breaks = seq(45, 75, by = 5)) +
        scale_y_continuous(limits = c(1, 20), breaks = seq(0, 20, by = 1)) +
        geom_vline(xintercept = mean(ciDat$Cscore), color = accent3, linewidth = 1) +
        theme(axis.text = element_text(size = 12),
              axis.title = element_text(size = 14, face = 'bold'))
    })
  })
  
```


```{r, echo = F}
tags$style(HTML(".js-irs-0 .irs-from, .js-irs-0 .irs-to, .js-irs-0 .irs-bar {background: #4CA384}"))
tags$style(HTML(".js-irs-1 .irs-from, .js-irs-1 .irs-to, .js-irs-1 .irs-bar {background: #4CA384}"))
tags$style(HTML(".js-irs-2 .irs-single, .js-irs-2 .irs-bar {background: #4CA384}"))
tags$style(HTML(".js-irs-3 .irs-single, .js-irs-3 .irs-bar {background: #4CA384}"))
tags$style(HTML(".js-irs-4 .irs-single, .js-irs-4 .irs-bar {background: #4CA384}"))
tags$style(HTML(".js-irs-5 .irs-single, .js-irs-5 .irs-bar {background: #4CA384}"))
tags$style(HTML(".js-irs-6 .irs-single, .js-irs-6 .irs-bar {background: #4CA384}"))
tags$style(HTML(".js-irs-7 .irs-single, .js-irs-7 .irs-bar {background: #4CA384}"))
tags$style(HTML(".js-irs-8 .irs-single, .js-irs-8 .irs-bar {background: #4CA384}"))
```

## Intro to Today's Lab

During today's lab, you'll apply the concepts discussed during this week's lecture. You'll also review a few concepts from last week's lecture. Each lab consists of a range of tasks, with corresponding questions you can answer. Please note that the questions are not required and not marked, although they do provide a helpful source of formative feedback that will help you gauge your understanding. 

Today, you'll be using interactive visualisations to demonstrate the concepts covered in the lecture. You will not be using SPSS. 

### Learning Objectives
At the end of this lab, you will be able to:

1. Understand the link between probability and inferential statistics
2. Describe the difference between effect sizes and $p$-values
3. Compute confidence intervals
4. Describe general requirements of a power analysis

## Computing the Area Under the Curve

Recall from the [lecture](https://mtruelovehill.github.io/PRM/Lectures/Week03_NHST_lecture.html#16) that the total area under the curve (AUC) allows us to compute the probability of an observation falling within a given range. Use the plot below to visualise how the AUC corresponds to different proportions of observations in a normal distribution.

```{r aucPlot, echo = F}

sliderInput('auc', 'Thresholds:', min=0, max = 1, step = .01, value = c(0,1), ticks = F)
plotOutput('aucPlot')

```
<br>

Now that you've started to get a grasp on what the area under the curve corresponds to, let's apply this to the example from last week.
Recall that you had a small sample of students who completed each of the three assessments. The optimal assessment was determined to be Assessment C. Imagine that the instructor adopted Assessment C for use in their course. The plot below reflects the probability distribution of assessment performance across all students on the course. 

```{r, echo = F}
sliderInput('score', 'Assessment Score:', min=31, max = 84, step = 1, value = c(31,84), ticks = F)
plotOutput('aucScorePlot')
```


```{r q1, echo = FALSE}

quiz(caption = 'Test Your Understanding',
     question_numeric('What was the maximum score on the assessment?',
                      answer(84, correct = T),
                      message = 'The maximum score was 84'),
     question_numeric('What percentage of students recieved an A (70+) on the assessment?',
                      answer(10, correct = T),
                      message = '10% of the sample scored 70 or above on the assessment.'),
     question_numeric('If we were to randomly select an exam booklet from the entire pile of assessments, what is the probability that the mark will be between 55 and 65?',
                      answer(40, correct = T),
                      answer(.40, correct = T),
                      message = 'The probability that we the mark will be between 55 and 65 is .40, or 40%'),
     question_numeric('The instructor wants to identify the students who scored in the lowest 5% so that they can check in and offer additional help. Fill in the blank: Students scoring __ and below will be contacted by the instructor.',
                      answer(43, correct = T),
                      message = '5% of the sample scored 43 or below.'),
     question_numeric('Assume this distribution is perfectly normal. What was the mean score on the assessment?',
                      answer(58, correct = T),
                      message = 'In a normal distribution, all observations are centred around the mean. The mean score on this assessment is 58, as 50% of students scored 58 or above, and 50% of students scored 58 or below.')
)
```


## The Null Distribution

When we run an inferential statistical test, it produces a test statistic. The null distribution for that test is the distribution of test statistics we would expect if the null hypothesis were true. If we run a statistical test and our statistic falls in the extreme region of the distribution, we consider our results to be significant and reject the null hypothesis (because our results would be really unusual if the null hypothesis were true!). See the [lecture](https://mtruelovehill.github.io/PRM/Lectures/Week03_NHST_lecture.html#26) if you need a refresher. 

Before running a test, we use $\alpha$ to determine our cutoff for significance. $\alpha$ refers to the proportion of values in the null distribution we consider to be extreme. In the plot below, the shaded region indicates the 'extreme' region associated with each value of $\alpha$. 


```{r, echo = F}
fluidRow(column(width = 2, 
                selectInput('alpha', 'Choose an alpha value:', choices = list('.05' = .05, '.01' = .01, '.001' = .001))))

plotOutput('alphaPlot')
```


```{r q2, echo = FALSE}

quiz(caption = 'Test Your Understanding',
     question('Given an alpha of .05, would a test statistic of -1.5 be significant?',
              answer('Yes'),
              answer('No', correct = T),
              message = 'A value of -1.5 would not fall past the alpha threshold, so would not be considered significant.'),
     question('Given an alpha of .001, would a test statistic of 3.5 be significant?',
              answer('Yes', correct = T),
              answer('No'),
              message = 'A value of 3.5 would fall past the alpha threshold and would be considered significant.'),
     question('Given an alpha of .01, would a test statistic of 2.5 be significant?',
              answer('Yes'),
              answer('No', correct = T),
              message = 'A value of 2.5 would not fall past the alpha threshold, so would not be considered significant.')
)
```


## Computing <i> p </i>

Recall from the [lecture](https://mtruelovehill.github.io/PRM/Lectures/Week03_NHST_lecture.html#37) that a $p$-value is the probability of obtaining a statistical test result at least as extreme as ours, given that the null hypothesis is true. Keep in mind that we're interested in the probability of any value **at least as extreme as our value**, regardless of whether it is positive or negative. This means that the $p$-value is calculated by summing the probability of scores more extreme than our value on **both** sides of the distribution. 

The plot below shows the probability of each shaded region, as well as a line representing the test statistic value. Use this plot to answer the following questions.

```{r, echo = F}
sliderInput('testStatistic', 'Value of Test Statistic:', min=-5, max = 5, step = .01, value = 0, ticks = F)
plotOutput('pPlot')
```


```{r q3, echo = FALSE}
quiz(caption = 'Test Your Understanding',
     question_numeric("What is the p-value associated with obtaining a test statistic at least as extreme as 2.25? Please report your answer to 3 decimal places.",
              answer(.024, correct = T),
              message = 'For a test statistic of 2.25, p = .024. Because you need the probability of values as extreme as yours (regardless of whether they are positive or negative) you must add the probabilities in each tail. .012 + .012 = .024'),
     question_numeric("What is the p-value associated with obtaining a test statistic at least as extreme as -1.36? Please report your answer to 3 decimal places.",
               answer(.174, correct = T),
              message = "For a test statistic of -1.50, p = .174 Because you need the probability of values as extreme as yours (regardless of whether they are positive or negative) you must add the probabilities in each tail. .087 + .087 = .174"),
     question_numeric('Which test statistic reflects the cutoff for the most extreme 5% of the data?',
              answer(-1.96, correct = T),
              answer(1.96, correct = T),
              message = "Because 5% is split between each tail of the distribution, any statistic with a corresponding probability of 2.5% or less would fall into the most extreme 5% of the values. The cutoff point is 1.96."),
     question('Which of the following test statistics are in the most extreme 1% of the data? Please select all that apply.',
              answer(-2.76, correct = T),
              answer(2.34),
              answer(1.74),
              answer(-2.58, correct = T),
              message = "Because 1% is split between each tail of the distribution, any statistic with a corresponding probability of .5% or less would fall into the most extreme 1% of the values.")
)
```


## Putting it all Together

In the last few tasks, we've broken down the general steps of an inferential statistical test. This gives you a glimpse of what's going on under the hood of SPSS. Now, let's put the steps together. 


Use the plot below to answer the following questions. The shaded area reflects $p$, the probability of getting a test statistic at least as extreme as yours given the null is true.

Please note that in the previous sections, $P$ (capital) reflected the probability of scores within each region. Here, $p$ (lowercase) refers to the $p$-value (the total probability of both shaded regions). 

```{r, echo = F}
sliderInput('testStat2', 'Value of Test Statistic:', min=-5, max = 5, step = .01, value = 0, ticks = F)
plotOutput('testPlot')
```



```{r q4, echo = FALSE}
quiz(caption = 'Test Your Understanding',
     question("If alpha = .001, is the p-value associated with a test statistic of 3.00 significant?",
              answer('Yes'),
              answer('No', correct = T),
              message = 'For a test statistic of 3.00, p = .002, which is more than the alpha threshold of .001, so we would not consider this result significant.'),
     question("If alpha = .01, is the p-value associated with a test statistic of -2.55 significant?",
               answer('Yes', correct = T),
              answer('No'),
              message = "For a test statistic of -2.55, p = .010. As this is equal to the alpha threshold of .01, this result would be significant. Or not. There is actually debate about this circumstance. Either answer is fine, because honestly, there is no meaningful difference between a p-value of .049 and .050, so your decision doesn't really matter anyways. However, we need some kind of threshold to avoid a slippery slope, so anything above your alpha is definitively nonsignificant.")
)
```

## Interpreting Effect Sizes

Although a $p$-value provides you with evidence of whether the null hypothesis can be rejected, it doesn't provide information about the strength of the effect being tested. Even weak relationships may be significant under certain circumstances. Significance does not guarantee an effect is meaningful. To get a better sense of this, we use effect size. Use the plot below to explore the relationships between means, standard deviations, effect size, sample size, and the $p$-value. While you interact with the plot, consider the following:

+ How does changing the difference between means affect the effect size? The $p$-value?

+ How does changing the standard deviation affect the effect size? The $p$-value?

+ How does changing the sample size affect the effect size? The $p$-value?

**Please note** that due to the way the data are simulated, there will not be a perfect negative or positive relationship between these values. It's more important that you grasp the general trend in their relationship than note the specific values being produced. The answers to the quiz questions will reflect this.

```{r effSizePlot, echo = F}
fluidRow(column(6,
                selectInput('meanDiff', 'Select the Difference between Group Means:', 
                            choices = list('.10' = .10, '.5' = .5, '1' = 1, '3' = 3, '5' = 5)),
                selectInput('n', 'Sample Size per Group:', 
                            choices = list('5' = 5, '30' = 50, '100' = 100, '500' = 500, '1000' = 1000, '5000' = 5000)),
                style='padding-left:50px; padding-right:10px; padding-bottom: 0px; padding-top: 0px'),
  column(6, sliderInput('sdG1', 'Pooled SD', min=1, max = 5, step = 1, value = 1, ticks = F),
         style='padding-left:50px; padding-right:30px; padding-bottom: 0px; padding-top: 0px'))

plotOutput('effectSizePlot')
```

```{r q5, echo = FALSE}
quiz(caption = 'Test Your Understanding',
     question("If the difference between means and the sample size is held constant, what generally happens to the effect size when the SD is increased?",
              answer('The effect size increases'),
              answer('The effect size decreases', correct = T),
              answer('The effect size stays about the same'),
              message = 'Increasing the standard deviation of will decrease the effect size. This is because the variability in the data is increased.'),
     question("If the group SD and the sample size is held constant, what generally happens to the effect size when the difference between means is increased?",
              answer('The effect size increases', correct = T),
              answer('The effect size decreases'),
              answer('The effect size stays about the same'),
              message = 'Increasing the difference between means indicates a stronger effect of the independent/grouping variable, so the effect size increases.'),
     question("If the difference between means and the group SD is held constant, what generally happens to the effect size when the sample size is increased?",
              answer('The effect size increases'),
              answer('The effect size decreases'),
              answer('The effect size stays about the same', correct = T),
              message = 'Increasing the sample size does not have much of an effect on the effect size.'),
     question("If the difference between means and the group SD is held constant, what generally happens to the p-value when the sample size is increased?",
              answer('The p-value increases'),
              answer('The p-value decreases', correct = T),
              answer('The p-value stays about the same'),
              message = 'Increasing the sample size can also decrease the p-value, causing results to appear more significant even if the effect is quite small.'))
```

## Compute a 95% Confidence Interval 

Recall from the [lecture](https://mtruelovehill.github.io/PRM/Lectures/Week03_NHST_lecture.html#47) that a 95% confidence level indicates that, were we to collect 100 samples and compute 95% confidence intervals each time, ~95 of those confidence intervals would be expected to contain the true population value. 

Here, we'll perform a simulation to illustrate this idea. 

Let's think back to our instructor, who adapted Assessment C for use in their course. The instructor would like to know the average  score for all students who have ever taken and will ever take the course. Although that's not possible, they can estimate this value by giving their current students the assessment and inspecting the average score.

```{r q6, echo = F}
quiz(caption = 'Test Your Understanding',
     question("In the above example, which of the following is the sample?",
              answer("All students who have ever taken and will ever take the instructor's course"),
              answer("The current students on the course", correct = T),
              message = 'All students who have ever taken/will ever take the course comprise the total population. The current students on the course comprise a sample of that population.'),
      question("In the above example, which of the following would be considered the population parameter?",
              answer("The mean score for all students who have ever taken and will ever take the course", correct = T),
              answer("The mean score for current students on the course"),
              message = 'The population parameter is the value we are attempting to estimate by using sample data. In this case, it is the mean score for all students who have ever taken or will ever take the course.')
     )
```


We often cannot know the true population parameter, but for the purposes of illustration, let's imagine that we have access to the performance of all past/present/future students who take this instructors course, and it looks something like this:

```{r, echo = F, message = F, warning = F}
ciDat <- read.csv('https://mtruelovehill.github.io/PRM/Data/ciDat.csv')

ggplot(ciDat, aes(Cscore)) + geom_histogram(fill = baseColor, color = accent3) + 
  xlab('Score') +
  theme(axis.text.x = element_text(size = 12),
          axis.title.x = element_text(size = 14, face = 'bold'),
          axis.text.y = element_blank(),
          axis.title.y = element_blank(),
          axis.ticks.y = element_blank()) +
  annotate('text', label = paste0('Mean = ', round(mean(ciDat$Cscore), 2)), x = 40, y = 700, size = 6)
  
```

```{r q7, echo = F}
quiz(caption = 'Test Your Understanding',
     question_numeric("What is the value of the population parameter in this example? Please round your answer to 2 decimal places.",
              answer(57.97, correct = T),
              message = 'The population parameter is the mean score on the assessment, 57.97.')
     )
```


To demonstrate what the confidence interval tells us, we need to take multiple samples from the population and calculate a confidence interval from each. Given what a 95% confidence interval is telling us, if we were to take 20 samples of 50 students on the course and compute a confidence interval with each, we would expect around 19 (95%) of those confidence intervals to include a value of 57.97. 

Use the button to generate 20 new samples. In the plot, the points represent the mean of each sample, and the error bars show the 95% confidence intervals. The true population mean is represented by the black line. Red error bars indicate confidence intervals that do not include the true population mean. If you were to press the button many times, eventually you would find that, on average, 95% of the confidence intervals contain the true population mean of 57.97.

```{r, echo = F}
actionButton("do", "Collect 20 Samples")
plotOutput('ciPlot')
```

You'll also note that while each sample's mean falls near the true population value, it is rare for a sample mean to perfectly capture the population mean. Confidence intervals are useful because our CI is more likely than the sample mean to capture the population mean.

```{r q8, echo = F}
quiz(caption = 'Test Your Understanding',
     question("Margot conducted a memory test on a sample of students, and found that the mean score was 75 points, with a 95% confidence interval of 65 - 85. Which of the following can she conclude?",
              answer('If Margot were to conduct this test over and over, 95% of the sample would always achieve a score between 65 and 85 points.'),
              answer('There is a .95 probability that the population mean is between 65 and 85 points.'),
              answer('If she calculated the 95% CI using the same method on a large number of samples, 95% of those CIs would contain the population mean. Therefore, she can be 95% confident that her CI contains the population mean.', correct = T),
              answer('She can be 95% confident that she has done the correct calculation.'),
              message = 'The population mean is fixed, so any individual confidence interval that has already been calculated either contains the popultion mean or does not contain it. However, if you calculate an X% confidence interval on multiple samples, then X% of those intervals will contain the population mean.'))
```

## Power Analyses

An a priori power analysis should be conducted when designing the study in order to determine the number of participants needed to detect the effect of interest. To run an a priori power analysis, you will need (at minimum) a power threshold, an alpha value, and an estimated effect size. 

In this course, we'll be using [WebPower](https://webpower.psychstat.org/wiki/models/index) to conduct our power analyses. We'll begin to conduct power analyses when we run specific statistical tests. Today, we'll focus on improving your understanding of the concept of statistical power. Have a look at the following power curves and spend some time trying to interpret them.

```{r, echo = F, fig.width = 8, fig.height=4}

powerDat <- read.csv('https://mtruelovehill.github.io/PRM/Data/powerDat.csv')
powerDat$EffectSize <- factor(powerDat$EffectSize, levels = c('strong', 'moderate', 'weak'))

(powPlot <- ggplot(powerDat, aes(SampleSize, Power, color = EffectSize)) + geom_point() +
  labs(x = 'Sample Size', y = 'Power', color = 'Effect Size') +
  scale_x_continuous(breaks = seq(0, 200, by = 20)) +
  scale_y_continuous(breaks = seq(0, 1, by = .1)) +
  scale_color_manual(values = c(accent3, accent1, baseColor), labels = c('Strong', 'Moderate', 'Weak')) +
  theme(axis.text = element_text(size = 14),
        axis.title = element_text(size = 16, face = 'bold'),
        legend.title = element_text(size = 14, face = 'bold'),
        legend.text = element_text(size = 12)))
```

As you interpret this plot, try to answer the following questions:

+ What is the effect of sample size on power?

+ What is the effect of effect size on power?

+ About how many participants are necessary to detect a moderate effect with 80% power? 

+ If you expect a weak effect but have only 50 participants, what is your likelihood of making a Type II error?

```{r q9, echo = FALSE}
 quiz(caption = 'Test Your Understanding',
      question('What is the traditional threshold for power that is considered acceptable?',
               answer('70%'),
               answer('80%', correct = T),
               answer('90%'),
               answer('100%'),
               message = 'Traditionally, a threshold of 80% is considered an acceptable level of power for a study.'),
      question('Which of the following is not considered when calculating power?',
               answer('Sample variance', correct = T),
               answer('Sample size'),
               answer('Significance level'),
               answer('Effect size'),
               random_answer_order = T,
               message = 'Power is affected by the significance level, sample size, and effect size. Each of these four values is a function of the remaining three.'),
      question('Which of the following would lead to an increase in statistical power? Please select all that apply.',
               answer('Decreasing alpha'),
               answer('Increasing effect size', correct = T),
               answer('Using a two-tailed test instead of a one-tailed test'),
               answer('Increasing the number of participants in the sample', correct = T),
               random_answer_order = T,
               message = 'Power has a positive relationship with all of the given values, so that as effect size, sample size, and alpha increase, so too does power. Decreasing any of these lowers power. Going from a one-tailed to a two-tailed test decreases alpha by splitting it between each tail, so this would also lower power.'),
      question('You run a power analysis and find that, for your analysis, you will need 136 participants to have 80% power to detect a moderate effect (d = .5). Which of the following statements are true? Please select all that apply.',
               answer('If you recruit fewer than 136 participants, your chances of detecting an existing effect will decrease.', correct = T),
               answer('If you recruit at least 136 participants, you have an 80% chance of getting statistically significant results.'),
               answer('If you recruit 136 participants, you have a 20% chance of making a Type II error.', correct = T),
               answer('If the actual effect in the population is weaker than d = .5, 136 participants will still provide sufficient power to detect it.'),
               answer('If the actual effect in the population is stronger than d = .5, 136 participants will still provide sufficient power to detect it.', correct = T),
               random_answer_order = T,
               message = 'You only have an 80% chance of getting statistically significant results IF the effect actually exists. Additionally, reducing the effect size decreases power, so you will need to increase your sample size in order to detect a smaller effect. Increasing the effect size increases power, so you will still be sufficiently powered to detect a larger effect without changing the sample.')
      )
```


<div class="container">
<details><summary><span style = "font-weight: bold; font-size: 16pt"> Click here for a hint </span></summary>

&beta;: the likelihood of making a Type II error

Power = 1 - &beta;

</details>
</div>

</br>

<div class="container">
<details><summary><span style = "font-weight: bold; font-size: 16pt"> Click here for the solution </span></summary>

<i><b> What is the effect of sample size on power? What is the effect of effect size on power? </i></b>

The sample size values can be found on the x-axis of the plot. As these values increase, you can see that power increases as well. Each line represents a different effect size strength. A strong effect size provides more power than a moderate effect size, which provides more power than a weak effect size. 

</br>
<i><b> About how many participants are necessary to detect a moderate effect with 80% power? </i></b>

```{r, echo = F, fig.width=6, fig.height=3}
powPlot + 
  geom_segment(x = 0, xend = 63, y = .8, yend = .8, linetype = 'dashed') +
  geom_segment(x = 63, xend = 63, y = 0, yend = .8, linetype = 'dashed') +
  geom_point(x = 63, y = .8, shape = 8, size = 2)
```

If we check the line associated with moderate strength, we can see that we need about 63 participants to achieve a power of 80%.

</br>

<i><b> If you expect a weak effect but have only 50 participants, what is your likelihood of making a Type II error? </i></b>

```{r, echo = F, fig.width=6, fig.height=3}
powPlot + 
  geom_segment(x = 0, xend = 50, y = .16, yend = .16, linetype = 'dashed') +
  geom_segment(x = 50, xend = 50, y = 0, yend = .16, linetype = 'dashed') +
  geom_point(x = 50, y = .16, shape = 8, size = 2)
```

By checking the line associated with a weak effect size, we can see that 50 participants is associated with a power of about .16. To calculate the chances of making a Type II error using our power value, we need to subtract .16 from 1. This leaves us with .84, meaning we have about an 84% chance of making a Type II error. In other words, we have a high likelihood of failing to identify an existing effect.

</details>
</div>
</br>



